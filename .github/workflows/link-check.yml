name: link-check
on:
  pull_request:
  push:
    branches: [ "main" ]

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Internal link audit (HTML)
        run: |
          python - << 'PY'
          import re, sys, json
          from pathlib import Path

          ROOT = Path('.').resolve()

          def is_external(u: str) -> bool:
            u = u.strip()
            return bool(re.match(r'^(?:https?:|mailto:|tel:|data:|javascript:|#)', u, re.I))

          def norm(u: str) -> str:
            u = u.strip().replace('\\', '/')
            u = u.split('#', 1)[0]
            u = u.split('?', 1)[0]
            while u.startswith('/'): u = u[1:]
            while u.startswith('./'): u = u[2:]
            while u.startswith('../'): u = u[3:]
            return re.sub(r'/+', '/', u)

          attr_re   = re.compile(r'''(?:\b(?:src|href|data-src|poster)\s*=\s* (["'])([^"']+)\1)''', re.I | re.VERBOSE)
          srcset_re = re.compile(r'''\bsrcset\s*=\s*(["'])([^"']+)\1''', re.I | re.VERBOSE)
          url_re    = re.compile(r'url\(\s*([^)]+?)\s*\)', re.I)

          problems = []

          for p in ROOT.rglob('*.html'):
            txt = p.read_text(encoding='utf-8', errors='ignore')

            for m in attr_re.finditer(txt):
              url = m.group(2).strip()
              if url and not url.startswith('#') and not is_external(url):
                n = norm(url)
                if not (ROOT / n).exists():
                  problems.append({'file': str(p), 'attr': 'attr', 'url': url, 'normalized': n})

            for m in srcset_re.finditer(txt):
              val = m.group(2)
              for part in [s.strip() for s in val.split(',') if s.strip()]:
                url = part.split()[0]
                if url and not url.startswith('#') and not is_external(url):
                  n = norm(url)
                  if not (ROOT / n).exists():
                    problems.append({'file': str(p), 'attr': 'srcset', 'url': url, 'normalized': n})

            for m in url_re.finditer(txt):
              url = m.group(1).strip().strip('"\'')
              if url and not url.startswith('#') and not is_external(url):
                n = norm(url)
                if not (ROOT / n).exists():
                  problems.append({'file': str(p), 'attr': 'style-url', 'url': url, 'normalized': n})

          if problems:
            print(f'Broken internal links: {len(problems)}')
            for r in problems[:100]:
              print(f"- {r['file']}: {r['attr']}='{r['url']}' â†’ {r['normalized']} (MISSING)")
            Path('link-report.json').write_text(json.dumps(problems, indent=2), encoding='utf-8')
            sys.exit(1)

          print('OK: no missing internal links')
          PY

      - name: Upload link report if failed
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: link-report
          path: link-report.json
